---
title: EchoSign - ASL Recognition
emoji: ğŸ¤Ÿ
colorFrom: blue
colorTo: green
sdk: streamlit
sdk_version: "1.51.0"
app_file: app.py
pinned: false
license: mit
python_version: "3.10"
---

# ğŸ¤Ÿ EchoSign - Real-time ASL Recognition

EchoSign is a real-time American Sign Language (ASL) recognition system that uses computer vision and machine learning to translate hand gestures into letters (A-Z).

## âœ¨ Features

- ğŸ¥ Real-time webcam-based hand tracking
- ğŸ¤– 97.32% accurate ML model using Random Forest
- ğŸŒ Web-based interface with Streamlit
- ğŸ“Š Live confidence scores
- ğŸ¯ Optimized for performance with MediaPipe Hands

## ğŸš€ Live Demo

**[Try EchoSign Live on Hugging Face](https://huggingface.co/spaces/YOUR_USERNAME/EchoSign)** _(Update with your HF username after deployment)_

## ğŸ› ï¸ Technology Stack

- **Computer Vision**: OpenCV, MediaPipe Hands
- **Machine Learning**: scikit-learn (Random Forest Classifier)
- **Web Framework**: Streamlit, streamlit-webrtc
- **Data Processing**: NumPy, Pandas

## ğŸ“¦ Installation

1. Clone the repository:
```bash
git clone https://github.com/Aanishnithin07/EchoSign.git
cd EchoSign
```

2. Create a virtual environment (Python 3.12):
```bash
python3.12 -m venv .venv-py312
source .venv-py312/bin/activate  # On Windows: .venv-py312\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install Git LFS (if cloning the model):
```bash
git lfs install
git lfs pull
```

## ğŸ® Usage

Run the Streamlit app locally:
```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

## ğŸ¯ How It Works

1. **Hand Tracking**: MediaPipe detects 21 hand landmarks in real-time
2. **Feature Extraction**: Landmark coordinates are normalized and flattened
3. **Prediction**: Random Forest model classifies the hand gesture
4. **Display**: Predicted letter and confidence score shown on screen

## ğŸ“Š Model Performance

- **Accuracy**: 97.32%
- **Training Samples**: 1,864 gestures
- **Features**: 42 normalized landmark coordinates
- **Classes**: 26 ASL letters (A-Z)

## ğŸ—ï¸ Project Structure

```
EchoSign/
â”œâ”€â”€ app.py                    # Streamlit web application
â”œâ”€â”€ phase1_hand_tracker.py    # Hand tracking module
â”œâ”€â”€ phase2_data_collector.py  # Data collection tool
â”œâ”€â”€ phase3_train_model.py     # Model training script
â”œâ”€â”€ phase4_realtime_test.py   # Real-time testing
â”œâ”€â”€ asl_model.joblib          # Trained ML model (Git LFS)
â”œâ”€â”€ asl_dataset.csv           # Training dataset
â””â”€â”€ requirements.txt          # Python dependencies
```

## ğŸ”§ Development Phases

1. **Phase 1**: Hand tracking with MediaPipe
2. **Phase 2**: Data collection (30 samples per letter)
3. **Phase 3**: Model training with Random Forest
4. **Phase 4**: Real-time testing
5. **Phase 5**: Web deployment with Streamlit
6. **Phase 6**: Cloud deployment

## ğŸŒ Deployment

Deployed on Streamlit Cloud for free public access.

## ğŸ“ License

MIT License - feel free to use this project for learning and development!

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

## ğŸ‘¨â€ğŸ’» Author

**Aanish Nithin**
- GitHub: [@Aanishnithin07](https://github.com/Aanishnithin07)

## ğŸ™ Acknowledgments

- MediaPipe by Google for hand tracking
- Streamlit for the amazing web framework
- scikit-learn for machine learning tools

---

â­ Star this repo if you found it helpful!
